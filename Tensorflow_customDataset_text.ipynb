{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "IIxJNnFkN3U8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "NsVuPZu4OVD0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ER_HYTjJEdF",
        "outputId": "1678b3c5-6e00-48d5-814c-012ab04106d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/danukhan/imdb-sentiment-dataset\n",
            "License(s): unknown\n",
            "Downloading imdb-sentiment-dataset.zip to /content\n",
            " 71% 36.0M/50.6M [00:00<00:00, 191MB/s] \n",
            "100% 50.6M/50.6M [00:00<00:00, 204MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d danukhan/imdb-sentiment-dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/imdb-sentiment-dataset.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "2YN2eGPvNu81"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pickle"
      ],
      "metadata": {
        "id": "ZGOEBxHqOnfl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = tf. data.TextLineDataset (\"imdb.csv\")\n",
        "ds_test = tf . data. TextLineDataset (\"imdb.csv\")"
      ],
      "metadata": {
        "id": "pTwCSipyPLeS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in ds_train.skip(1) .take (5): #skip first row that contain name\n",
        "  print (tf.strings.split(line, ',', maxsplit=4)) #maxsplit so that review want get split by comma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-C8-mfvPjGg",
        "outputId": "2179e648-bdc0-4f74-f1e5-60d7ee42f164"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'0' b'test' b'neg' b'0_2.txt'\n",
            " b'\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner\\'s character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he\\'s better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher\\'s ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'1' b'test' b'neg' b'10000_4.txt'\n",
            " b'\"This is an example of why the majority of action films are the same. Generic and boring, there\\'s really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who\\'ve each proven many times over that they are capable of acting, and acting well. Don\\'t bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T\\'s horribly cliched dialogue alone makes this film grate at the teeth, and I\\'m still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I\\'ve seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'2' b'test' b'neg' b'10001_1.txt'\n",
            " b'\"First of all I hate those moronic rappers, who could\\'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like clich\\xc3\\x83\\xc2\\x83\\xc3\\x82\\xc2\\xa9\\'e version of gangsters.<br /><br />The movie doesn\\'t take more than five minutes to explain what is going on before we\\'re already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain.<br /><br />Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did\\'nt like him right from the start.<br /><br />The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve.<br /><br />The only two black people that can act is the homeless guy and the junkie but they\\'re actors by profession, not annoying ugly brain dead rappers.<br /><br />Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'3' b'test' b'neg' b'10002_3.txt'\n",
            " b'\"Not even the Beatles could write songs everyone liked, and although Walter Hill is no mop-top he\\'s second to none when it comes to thought provoking action movies. The nineties came and social platforms were changing in music and film, the emergence of the Rapper turned movie star was in full swing, the acting took a back seat to each man\\'s overpowering regional accent and transparent acting. This was one of the many ice-t movies i saw as a kid and loved, only to watch them later and cringe. Bill Paxton and William Sadler are firemen with basic lives until a burning building tenant about to go up in flames hands over a map with gold implications. I hand it to Walter for quickly and neatly setting up the main characters and location. But i fault everyone involved for turning out Lame-o performances. Ice-t and cube must have been red hot at this time, and while I\\'ve enjoyed both their careers as rappers, in my opinion they fell flat in this movie. It\\'s about ninety minutes of one guy ridiculously turning his back on the other guy to the point you find yourself locked in multiple states of disbelief. Now this is a movie, its not a documentary so i wont waste my time recounting all the stupid plot twists in this movie, but there were many, and they led nowhere. I got the feeling watching this that everyone on set was sord of confused and just playing things off the cuff. There are two things i still enjoy about it, one involves a scene with a needle and the other is Sadler\\'s huge 45 pistol. Bottom line this movie is like domino\\'s pizza. Yeah ill eat it if I\\'m hungry and i don\\'t feel like cooking, But I\\'m well aware it tastes like crap. 3 stars, meh.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'4' b'test' b'neg' b'10003_3.txt'\n",
            " b'\"Brass pictures (movies is not a fitting word for them) really are somewhat brassy. Their alluring visual qualities are reminiscent of expensive high class TV commercials. But unfortunately Brass pictures are feature films with the pretense of wanting to entertain viewers for over two hours! In this they fail miserably, their undeniable, but rather soft and flabby than steamy, erotic qualities non withstanding.<br /><br />Senso \\'45 is a remake of a film by Luchino Visconti with the same title and Alida Valli and Farley Granger in the lead. The original tells a story of senseless love and lust in and around Venice during the Italian wars of independence. Brass moved the action from the 19th into the 20th century, 1945 to be exact, so there are Mussolini murals, men in black shirts, German uniforms or the tattered garb of the partisans. But it is just window dressing, the historic context is completely negligible.<br /><br />Anna Galiena plays the attractive aristocratic woman who falls for the amoral SS guy who always puts on too much lipstick. She is an attractive, versatile, well trained Italian actress and clearly above the material. Her wide range of facial expressions (signalling boredom, loathing, delight, fear, hate ... and ecstasy) are the best reason to watch this picture and worth two stars. She endures this basically trashy stuff with an astonishing amount of dignity. I wish some really good parts come along for her. She really deserves it.\"'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_train(line):\n",
        "\n",
        "  split_line = tf.strings.split(line, \",\", maxsplit=4)\n",
        "  dataset_belonging = split_line[1] # train, test\n",
        "  sentiment_category = split_line [2] # pos, neg, unsup\n",
        "\n",
        "  return (\n",
        "      True\n",
        "      if dataset_belonging == 'train' and sentiment_category != 'Unsup'\n",
        "      else False\n",
        "  )\n",
        "\n",
        "def filter_test(line):\n",
        "\n",
        "  split_line = tf.strings.split(line, \",\", maxsplit=4)\n",
        "  dataset_belonging = split_line[1] # train, test\n",
        "  sentiment_category = split_line [2] # pos, neg, unsup\n",
        "\n",
        "  return (\n",
        "      True\n",
        "      if dataset_belonging == 'test' and sentiment_category != 'Unsup'\n",
        "      else False\n",
        "  )"
      ],
      "metadata": {
        "id": "NIWmskulPnGw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = tf. data.TextLineDataset (\"imdb.csv\").filter(filter_train)\n",
        "ds_test = tf . data. TextLineDataset (\"imdb.csv\").filter(filter_test)"
      ],
      "metadata": {
        "id": "qLYpy_HQRk7y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in ds_train.skip(1) .take (5): #skip first row that contain name\n",
        "  print (tf.strings.split(line, ',', maxsplit=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA4cnifHSD9b",
        "outputId": "4bdc026b-ab1a-476b-a8dd-4ff21492a539"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'25001' b'train' b'neg' b'10000_4.txt'\n",
            " b'\"Airport \\'77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP\\'s to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice\\'s Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor\\'s as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it\\'s predecessors I can\\'t say Airport \\'77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn\\'t he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it\\'s a decent idea that could have made for a great little disaster flick but bad unsympathetic character\\'s, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there\\'s not as much urgency as I thought there should have been. Even when the Navy become involved things don\\'t pick up that much with a few shots of huge ships & helicopters flying about but there\\'s just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport \\'77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character\\'s, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport \\'77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren\\'t great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award\\'s Hall of Shame although I can think of lots of worse films than this so I reckon that\\'s a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn\\'t great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport \\'77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn\\'t help though & a film about a sunken plane just shouldn\\'t be this boring or lethargic. Followed by The Concorde ... Airport \\'79 (1979).\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'25002' b'train' b'neg' b'10001_4.txt'\n",
            " b'\"This film lacked something I couldn\\'t put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don\\'t know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don\\'t forget it was nominated for an Oscar, so judge for yourself.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'25003' b'train' b'neg' b'10002_1.txt'\n",
            " b'\"Sorry everyone,,, I know this is supposed to be an \"\"art\"\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i\\'m no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'25004' b'train' b'neg' b'10003_1.txt'\n",
            " b'\"When I was little my parents took me along to the theater to see Interiors. It was one of many movies I watched with my parents, but this was the only one we walked out of. Since then I had never seen Interiors until just recently, and I could have lived out the rest of my life without it. What a pretentious, ponderous, and painfully boring piece of 70\\'s wine and cheese tripe. Woody Allen is one of my favorite directors but Interiors is by far the worst piece of crap of his career. In the unmistakable style of Ingmar Berman, Allen gives us a dark, angular, muted, insight in to the lives of a family wrought by the psychological damage caused by divorce, estrangement, career, love, non-love, halitosis, whatever. The film, intentionally, has no comic relief, no music, and is drenched in shadowy pathos. This film style can be best defined as expressionist in nature, using an improvisational method of dialogue to illicit a \"\"more pronounced depth of meaning and truth\"\". But Woody Allen is no Ingmar Bergman. The film is painfully slow and dull. But beyond that, I simply had no connection with or sympathy for any of the characters. Instead I felt only contempt for this parade of shuffling, whining, nicotine stained, martyrs in a perpetual quest for identity. Amid a backdrop of cosmopolitan affluence and baked Brie intelligentsia the story looms like a fart in the room. Everyone speaks in affected platitudes and elevated language between cigarettes. Everyone is \"\"lost\"\" and \"\"struggling\"\", desperate to find direction or understanding or whatever and it just goes on and on to the point where you just want to slap all of them. It\\'s never about resolution, it\\'s only about interminable introspective babble. It is nothing more than a psychological drama taken to an extreme beyond the audience\\'s ability to connect. Woody Allen chose to make characters so immersed in themselves we feel left out. And for that reason I found this movie painfully self indulgent and spiritually draining. I see what he was going for but his insistence on promoting his message through Prozac prose and distorted film techniques jettisons it past the point of relevance. I highly recommend this one if you\\'re feeling a little too happy and need something to remind you of death. Otherwise, let\\'s just pretend this film never happened.\"'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'25005' b'train' b'neg' b'10004_3.txt'\n",
            " b'\"\"\"It appears that many critics find the idea of a Woody Allen drama unpalatable.\"\" And for good reason: they are unbearably wooden and pretentious imitations of Bergman. And let\\'s not kid ourselves: critics were mostly supportive of Allen\\'s Bergman pretensions, Allen\\'s whining accusations to the contrary notwithstanding. What I don\\'t get is this: why was Allen generally applauded for his originality in imitating Bergman, but the contemporaneous Brian DePalma was excoriated for \"\"ripping off\"\" Hitchcock in his suspense/horror films? In Robin Wood\\'s view, it\\'s a strange form of cultural snobbery. I would have to agree with that.\"'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "# 1. Create vocabulary\n",
        "# 2. Numericalize text str -> indices (TokenTextEncoder)\n",
        "# 3. Pad the batches so we can send in to an RNN for example"
      ],
      "metadata": {
        "id": "x_Fef7uRSHSq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "CPxKrH_5WJ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()"
      ],
      "metadata": {
        "id": "JiFQrzBEXfiw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary (ds_train, threshold=200):\n",
        "  \"\"\" Build a vocabulary \"\"\"\n",
        "  frequencies = {}\n",
        "  vocabulary = set()\n",
        "  vocabulary.update ([\"sostoken\" ])\n",
        "  vocabulary. update ([\"eostoken\"])\n",
        "\n",
        "  for line in ds_train.skip (1):\n",
        "    split_line = tf.strings.split(line, \",\", maxsplit=4)\n",
        "    review = split_line[4]\n",
        "    tokenized_text = tokenizer.tokenize(review.numpy().lower())\n",
        "\n",
        "    for word in tokenized_text:\n",
        "      if word not in frequencies:\n",
        "        frequencies [word] = 1\n",
        "      else:\n",
        "        frequencies [word] += 1\n",
        "\n",
        "      # if we've reached the threshold\n",
        "      if frequencies [word] == threshold:\n",
        "        vocabulary.update(tokenized_text)\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "6LiFFzVlSuv4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = build_vocabulary(ds_train)"
      ],
      "metadata": {
        "id": "joAiNvzaWq5_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary and save it to vocabulary.obj\n",
        "vocab_file = open ('vocabulary.obj','wb')\n",
        "pickle. dump (vocabulary, vocab_file)"
      ],
      "metadata": {
        "id": "jnJbcYnYYX8q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the vocabulary\n",
        "#vocab_file = open (\"vocabulary .obj\",\"rb\")\n",
        "#vocabulary = pickle. load (vocab_file)"
      ],
      "metadata": {
        "id": "worF7p6QY1bn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tfds.deprecated.text.TokenTextEncoder(\n",
        "    list (vocabulary),\n",
        "    oov_token=\"<UNK>\",\n",
        "    lowercase=True,\n",
        "    tokenizer=tokenizer)\n",
        "\n",
        "def my_encoder (text_tensor, label):\n",
        "  encoded_text = encoder. encode (text_tensor.numpy())\n",
        "  return encoded_text, label"
      ],
      "metadata": {
        "id": "SGZZEKZhY36r"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_map_fn (line):\n",
        "  split_line = tf.strings .split(line, \",\", maxsplit=4)\n",
        "  label_str = split_line [2] # neg, pos\n",
        "  review = \"sostoken \" + split_line [4] + \" eostoken\"\n",
        "  label = 1 if label_str == \"pos\" else 0\n",
        "  (encoded_text, label) = tf.py_function (\n",
        "      my_encoder,\n",
        "      inp=[review, label],\n",
        "      Tout=(tf.int64, tf.int32),\n",
        "  ) #for computation graph we need to do this\n",
        "  encoded_text.set_shape ([None]) #for computation graph we need to do this, review length can vary\n",
        "  label.set_shape([]) #as label will be a list\n",
        "\n",
        "  return encoded_text, label"
      ],
      "metadata": {
        "id": "jdeQUtE9abDP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data. experimental.AUTOTUNE\n",
        "ds_train = ds_train.map (encode_map_fn, num_parallel_calls=AUTOTUNE) . cache ()\n",
        "ds_train = ds_train.shuffle (25000)\n",
        "ds_train = ds_train.padded_batch (32, padded_shapes= ([None], ())) #padded shape [none] as we dont know lendth of largest in bact\n",
        "#and () fir label"
      ],
      "metadata": {
        "id": "KbasP_QcblT_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test = ds_test.map (encode_map_fn)\n",
        "ds_test = ds_test.padded_batch (32, padded_shapes=([None], ()))"
      ],
      "metadata": {
        "id": "14ytTh8MblSE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras. Sequential(\n",
        "    [\n",
        "        layers.Masking(mask_value=0),\n",
        "        layers.Embedding(input_dim=len(vocabulary)+2, output_dim=32,),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "EdeuQAdyblPF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model. compile (\n",
        "    loss = keras. losses. BinaryCrossentropy (from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam (3e-4, clipnorm=1), metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit (ds_train, epochs=15, verbose=2)\n",
        "model.evaluate(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGXdEyEBblMi",
        "outputId": "999044bd-646d-4d31-fbe1-9bf54e09e68b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'embedding_1' (of type Embedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 - 145s - 62ms/step - accuracy: 0.8333 - loss: 0.4006\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 - 68s - 29ms/step - accuracy: 0.8333 - loss: 0.4026\n",
            "Epoch 3/15\n",
            "2344/2344 - 72s - 31ms/step - accuracy: 0.8334 - loss: 0.3945\n",
            "Epoch 4/15\n",
            "2344/2344 - 80s - 34ms/step - accuracy: 0.8335 - loss: 0.3818\n",
            "Epoch 5/15\n",
            "2344/2344 - 83s - 35ms/step - accuracy: 0.8402 - loss: 0.3529\n",
            "Epoch 6/15\n",
            "2344/2344 - 80s - 34ms/step - accuracy: 0.8515 - loss: 0.3309\n",
            "Epoch 7/15\n",
            "2344/2344 - 76s - 32ms/step - accuracy: 0.8594 - loss: 0.3167\n",
            "Epoch 8/15\n",
            "2344/2344 - 72s - 31ms/step - accuracy: 0.8661 - loss: 0.3032\n",
            "Epoch 9/15\n",
            "2344/2344 - 84s - 36ms/step - accuracy: 0.8702 - loss: 0.2935\n",
            "Epoch 10/15\n",
            "2344/2344 - 66s - 28ms/step - accuracy: 0.8749 - loss: 0.2852\n",
            "Epoch 11/15\n",
            "2344/2344 - 79s - 34ms/step - accuracy: 0.8781 - loss: 0.2753\n",
            "Epoch 12/15\n",
            "2344/2344 - 83s - 35ms/step - accuracy: 0.8856 - loss: 0.2629\n",
            "Epoch 13/15\n",
            "2344/2344 - 82s - 35ms/step - accuracy: 0.8871 - loss: 0.2567\n",
            "Epoch 14/15\n",
            "2344/2344 - 83s - 35ms/step - accuracy: 0.8904 - loss: 0.2526\n",
            "Epoch 15/15\n",
            "2344/2344 - 62s - 27ms/step - accuracy: 0.8966 - loss: 0.2394\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.8486 - loss: 0.4193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3289943933486938, 0.5113999843597412]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for large dataset that is in multiple files we can do"
      ],
      "metadata": {
        "id": "c5dcErq3eEcU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = ['test_examplel.csv','test_example2.csv', 'test_example3.csv']\n",
        "\n",
        "dataset = tf.data. TextLineDataset(file_names)"
      ],
      "metadata": {
        "id": "F07DKBz1fCuZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#or if want different preprocessing\n",
        "\n",
        "dataset1 = tf .data. TextLineDataset ('test_example1.cs').skip (1)#.map (preprocess\n",
        "dataset2 = tf .data. TextLineDataset ('test_example2csv') .skip (1)#.map (preprocess\n",
        "dataset3 = tf .data. TextLineDataset ('test_example.csv') .skip (1)# .map (preprocess\n",
        "dataset = dataset1. concatenate (dataset2). concatenate (dataset3)"
      ],
      "metadata": {
        "id": "D3SmYA7ZfTxW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now if we have trasnlation task and two files contaning dataset for each lang then\n",
        "\n",
        "tokenizer = tfds.deprecated.text.Tokenizer ()\n",
        "english = tf .data. TextLineDataset (\"english.csv\")\n",
        "swedish = tf .data. TextLineDataset (\"swedish.csv\")\n",
        "dataset = tf .data. Dataset. zip ((english, swedish))"
      ],
      "metadata": {
        "id": "LFEY53COfpTP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for eng, swe in dataset.skip (1):\n",
        "  print(tokenizer.tokenize (eng .numpy ()))\n",
        "  print(tokenizer.tokenize (swe.numpy () . decode (\"UTF-8\"))) #we decode this as this lang contain special chars"
      ],
      "metadata": {
        "id": "3qGaqxVvgok5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJr4S_Q-gvFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}